{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637c4cc0",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers with Amazon SageMaker and Multi-Container Endpoints\n",
    "### Deploy multiple Transformer models to the same Amazon SageMaker Infrastructure\n",
    "\n",
    "\n",
    "Welcome to this getting started guide. We will use the Hugging Face Inference DLCs and Amazon SageMaker to deploy multiple transformer models as [Multi-Container Endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html). \n",
    "Amazon SageMaker Multi-Container Endpoint is an inference option to deploy multiple containers (multiple models) to the same SageMaker real-time endpoint. These models/containers can be accessed individually or in a pipeline. Amazon SageMaer [Multi-Container Endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html) can be used to improve endpoint utilization and optimize costs. An example for this is **time zone differences**, the workload for model A (U.S) is mostly at during the day and the workload for model B (Germany) is mostly during the night, you can deploy model A and model B to the same SageMaker endpoint and optimize your costs. \n",
    "\n",
    "_**NOTE:** As the time of writing this only `CPU` Instances are supported for Multi-Container Endpoint._\n",
    "\n",
    "\n",
    "![mce](imgs/mce.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7b101",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions\n",
    "\n",
    "_NOTE: You can run this demo in Sagemaker Studio, your local machine, or Sagemaker Notebook Instances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "assert sagemaker.__version__ >= \"2.75.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "### Permissions\n",
    "\n",
    "_If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1f104",
   "metadata": {},
   "source": [
    "## Multi-Container Endpoint creation\n",
    "\n",
    "As the time of writing this does the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) not support Multi-Container Endpoint deployments. That's why we are going to use `boto3` to create the endpoint.\n",
    "\n",
    "The first step though is to use the SDK to get our container uris for the Hugging Face Inference DLCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36248c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "hf_inference_dlc = image_uris.retrieve(framework='huggingface', \n",
    "                                region=region, \n",
    "                                version='4.12.3', \n",
    "                                image_scope='inference', \n",
    "                                base_framework_version='pytorch1.9.1', \n",
    "                                py_version='py38', \n",
    "                                container_version='ubuntu20.04', \n",
    "                                instance_type='ml.c5.xlarge')\n",
    "hf_inference_dlc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23091863",
   "metadata": {},
   "source": [
    "### Define Hugging Face models\n",
    "\n",
    "As a next step we need to define the models we want to deploy to our multi-container endpoint. To stick with our example from the introduction we are going to deploy a english sentiment-classification model and a german sentiment-classification model. For the english model we will use [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and for the german model we will use [oliverguhr/german-sentiment-bert](https://huggingface.co/oliverguhr/german-sentiment-bert). \n",
    "Similar to the endpoint creation with the SageMaker SDK do we need to provide the \"Hub\" configrations for the models as `HF_MODEL_ID` and `HF_TASK`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bf7e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english model\n",
    "english_model = {\n",
    "    'Image': ecr_image,\n",
    "    'ContainerHostname': 'english_model',\n",
    "    'Environment': {\n",
    "\t    'HF_MODEL_ID':'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "\t    'HF_TASK':'text-classification'\n",
    "    }\n",
    "}\n",
    "\n",
    "# german model\n",
    "german_model = {\n",
    "    'Image': ecr_image,\n",
    "    'ContainerHostname': 'german_model',\n",
    "    'Environment': {\n",
    "\t    'HF_MODEL_ID':'oliverguhr/german-sentiment-bert',\n",
    "\t    'HF_TASK':'text-classification'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set the Mode parameter of the InferenceExecutionConfig field to Direct for direct invocation of each container,\n",
    "# or Serial to use containers as an inference pipeline. The default mode is Serial.\n",
    "inferenceExecutionConfig = {\"Mode\": \"Direct\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21708dc",
   "metadata": {},
   "source": [
    "## Create Multi-Container Endpoint\n",
    "\n",
    "After we defined our model configuration we can deploy our endpoint. To create/deploy a real-time endpoint with `boto3` you need to create a \"SageMaker Model\", a \"SageMaker Endpoint Configuration\" and a \"SageMaker Endpoint\". The \"SageMaker Model\" contains our multi-container configuration including our two models. The \"SageMaker Endpoint Configuration\" contains the configuration for the endpoint. The \"SageMaker Endpoint\" is the actual endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1800c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"multi-container-sentiment\"\n",
    "instance_type =  \"ml.c5.4xlarge\"\n",
    "\n",
    "\n",
    "# create SageMaker Model\n",
    "sm_client.create_model(\n",
    "    ModelName        = f\"{deployment_name}-model\",\n",
    "    InferenceExecutionConfig = inferenceExecutionConfig,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers       = [english_model, german_model]\n",
    "    )\n",
    "\n",
    "# create SageMaker Endpoint configuration\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName= f\"{deployment_name}-config\",\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ModelName\":  f\"{deployment_name}-model\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# create SageMaker Endpoint configuration\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName= f\"{deployment_name}-ep\", EndpointConfigName=f\"{deployment_name}-config\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5d132",
   "metadata": {},
   "source": [
    "this will take a few minutes to deploy. You can check the console to see if the endpoint is in service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b0ecf",
   "metadata": {},
   "source": [
    "## Invoke Multi-Container Endpoint\n",
    "\n",
    "To invoke the our multi-container endpoint we can either use `boto3` or any other AWS SDK or the Amazon SageMaker SDK. We are going to test both ways and also do some light load testing to take a look at the performance of our endpoint in cloudwatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afe9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_payload={\"inputs\":\"This is a great way for saving money and optimizing my resources.\"}\n",
    "\n",
    "german_payload={\"inputs\":\"Diese Methode wird mir in Zukunft helfen Kosten zu sparen und meine Ressourcen zu optimieren.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cb5d3",
   "metadata": {},
   "source": [
    "### Sending requests with `boto3`\n",
    "\n",
    "To send requests to our models we will use the `sagemaker-runtime` with the `invoke_endpoint` method. Compared to sending regular requests to a single-container endpoint we are passing `TargetContainerHostname` as additional information to point to the container, which should recieve the request. In our case this is either `english_model` or `german_model`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90c26a",
   "metadata": {},
   "source": [
    "#### `english_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c91d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# create client\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# send request to first container (bi-encoder)\n",
    "response = invoke_client.invoke_endpoint(\n",
    "    EndpointName=f\"{deployment_name}-ep\",\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"english_model\",\n",
    "    Body=json.dumps(english_payload),\n",
    ")\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4d8a7",
   "metadata": {},
   "source": [
    "#### `german_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6778017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996057152748108}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# create client\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# send request to first container (bi-encoder)\n",
    "response = invoke_client.invoke_endpoint(\n",
    "    EndpointName=f\"{deployment_name}-ep\",\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"german_model\",\n",
    "    Body=json.dumps(german_payload),\n",
    ")\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52654f20",
   "metadata": {},
   "source": [
    "### Sending requests with `HuggingFacePredictor`\n",
    "\n",
    "The Python SageMaker SDK can not be used for deploying Multi-Container Endpoints but can be used to invoke/send requests to those. We will use the `HuggingFacePredictor` to send requests to the endpoint, where we also pass the `TargetContainerHostname` as additional information to point to the container, which should recieve the request. In our case this is either `english_model` or `german_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e46df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "\n",
    "# predictor\n",
    "predictor = HuggingFacePredictor(f\"{deployment_name}-ep\")\n",
    "\n",
    "# english request\n",
    "en_res = predictor.predict(english_payload, initial_args={\"TargetContainerHostname\":\"english_model\"})\n",
    "print(en_res)\n",
    "\n",
    "# german request\n",
    "de_res = predictor.predict(german_payload, initial_args={\"TargetContainerHostname\":\"german_model\"})\n",
    "print(de_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887e2ed",
   "metadata": {},
   "source": [
    "### Load testing the multi-container endpoint\n",
    "\n",
    "As mentioned we are doing some light load-testing, meaning sending a few alternating requests to the containers and looking at the latency in cloudwatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "  predictor.predict(english_payload, initial_args={\"TargetContainerHostname\":\"english_model\"})\n",
    "  predictor.predict(german_payload, initial_args={\"TargetContainerHostname\":\"german_model\"})\n",
    "  \n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcfd17a",
   "metadata": {},
   "source": [
    "## Delete the Multi-Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a73a4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
